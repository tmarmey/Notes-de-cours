\documentclass[12pt,a4paper]{article}
\usepackage[utf8]{inputenc}
\usepackage{graphicx}
\graphicspath{{../Images/}}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{hyperref}
\usepackage[margin=1in]{geometry}
\usepackage{subfig}
\usepackage{float}
\usepackage{xcolor}
\usepackage{listings}
\definecolor{dkgreen}{rgb}{0,0.6,0}
\definecolor{gray}{rgb}{0.5,0.5,0.5}
\definecolor{mauve}{rgb}{0.58,0,0.82}

\lstset{frame=tb,
  backgroundcolor=\color[rgb]{0.93,0.93,0.93},
  language=Python,
  aboveskip=3mm,
  belowskip=3mm,
  showstringspaces=false,
  columns=flexible,
  basicstyle={\small\ttfamily},
  numbers=none,
  numberstyle=\tiny\color{gray},
  keywordstyle=\color{blue},
  commentstyle=\color{dkgreen},
  stringstyle=\color{mauve},
  breaklines=true,
  breakatwhitespace=true,
  tabsize=3
}

\author{Thibaut Marmey}

\title{Notes de cours CADL - session-2}

\begin{document}
	\maketitle

\begin{scriptsize} \begin{itemize}
\item The basic components of a neural network
\item How to use gradient descent to optimize parameters of a neural network
\item How to create a neural network for performing regression
\end{itemize}\end{scriptsize}

\begin{normalsize}
\tableofcontents
\end{normalsize}

\section{Introduction}
\subsection{Generalities}
\begin{itemize}
\item Use data and gradient descent to teach the network what the values of the parameters of the network should be.
\item Idea of machine learning : letting the machine learn from the data.
\item We are interested in letting the computer figure out what representations it needs in order to better describe the data and some objective we've defined.
\end{itemize}
\subsection{Gradient Descent}
\begin{itemize}
\item Operation of the network are meant to transform the input data into something meaningful that we want the network to learn about.
\item Parameters of the NW are random so output is random as well.
\item If we need specific output, we can use "Gradient Descent" : way to optimize set of parameters.
\end{itemize}
\subsection{Defining Cost}
\begin{itemize}
\item Mesure of the "error"
\item Exemple : recognize apple or orange. Random network spit ou random 0s or 1s for apples and oranges. We can define :
\begin{itemize}
\item if the network predicts a 0 for an orange, then the error is 0. If the network predicts a 1 for an orange, then the error is 1. 
\item And vice-versa for apples. If it spits out a 1 for an apple, then the error is 0. If it spits out a 0 for an apple, then the error is 1. 
\end{itemize}
\item Defining error in terms of our parameters :
\begin{equation} error = network(image) - true\_label 
\end{equation}
\begin{equation} network(image) = predicted\_label
\end{equation}
\begin{equation} E = f(X) - y
\end{equation}
\end{itemize}
\subsection{Minimizing Error}
\begin{itemize}
\item Feed the network many images (100 for e.g) to see what the network is doing on average.
\item Changing network's parameters can have effect on the error.
\item The error provides a "training signal" or a measure of the "loss" or our network.
\item Assumptions in assuming our funtion is continuous and differentiable. 
\item Gradiant descent in a nutshell : "Error", "Cost", "Loss", or "Training Signal"
\end{itemize}
\subsection{Backpropagation}
\begin{itemize}
\item  The gradient is just saying, how does the error change at the current set of parameters.
\item To figure out what is the gradiet we use backpropagation. Whatever differences that output has with the output we wanted it to have, gets \textit{backpropagated} to every single param in our network.
\item Backprop is an effective way to find the gradient. Uses the \textit{chain rule} to find the gradian of the error.
\item \textit{y = mx + b} linear function. The slope or gradian is \textit{m}.
\item The process described : 
\begin{equation} \theta = \theta - \eta * \nabla_\theta * J(\theta)
\end{equation}
\begin{itemize}
\item $\theta$ : parameters
\item $\eta$ : gradient, with repect to parameters $\theta$, $\nabla_\theta$
\item J : error
\item $\eta$ : learning rate describes how far along this gradient we should travel, typically value between 0.01 to 0.00001
\end{itemize}
\end{itemize}
\subsection{Local Minima/Optima}
\begin{itemize}
\item 
\end{itemize}

\end{document}
